{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from canlpy.helpers.knowbert_loaders import load_wiki_model_and_batchifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "470105it [00:49, 9569.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model archives\n",
      "duplicate_mentions_cnt:  6777\n",
      "end of p_e_m reading. wall time: 1.276975897947947  minutes\n",
      "p_e_m_errors:  0\n",
      "incompatible_ent_ids:  0\n"
     ]
    }
   ],
   "source": [
    "wiki_model,wiki_batcher = load_wiki_model_and_batchifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlp/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input\n",
      "\n",
      "Tokens shape torch.Size([1, 8])\n",
      "Tokenized sentence tensor([[ 101, 3000, 2003, 2284, 1999,  103, 1012,  102]])\n",
      "Segment ids shape: torch.Size([1, 8])\n",
      "Candidate entity_priors shape: torch.Size([1, 3, 30])\n",
      "Candidate entities ids shape: torch.Size([1, 3, 30])\n",
      "Candidate span shape: torch.Size([1, 3, 2])\n",
      "Candidate segments_ids shape: torch.Size([1, 3])\n",
      "\n",
      "Output\n",
      "\n",
      "Output wordnet entity_attention_probs shape: torch.Size([1, 4, 8, 3])\n",
      "Output wordnet linking_scores shape: torch.Size([1, 3, 30])\n",
      "Output loss: 0.0\n",
      "Pooled output shape: torch.Size([1, 768])\n",
      "Contextual embeddings: torch.Size([1, 8, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Antoine/Documents/EPFL/MA2/NLP/canlpy/canlpy/core/components/fusion/knowbert_fusion/span_extractor.py:309: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1391.)\n",
      "  masked_vector = vector.masked_fill((1 - mask).byte(), mask_fill_value)\n"
     ]
    }
   ],
   "source": [
    "sentences = [[\"Paris is located in [MASK].\", \"Michael [MASK] is a great music singer\"],\n",
    "                [\"The Louvre contains the Mona Lisa\", \"The Amazon river is in Brazil\"],\n",
    "                \"Donald Duck is a cartoon character\",\n",
    "                [\"Hayao Miyazaki is the co-founder of Studio Ghibli and a renowned anime film maker\",\n",
    "                \"The Alpine ibex is one of Switzerland's most famous animal along its grazing cows\"]]\n",
    "\n",
    "sentence = [\"Paris is located in [MASK].\"]\n",
    "\n",
    "\n",
    "for batch in wiki_batcher.iter_batches(sentence, verbose=False):\n",
    "\n",
    "    print(f\"\\nInput\\n\")\n",
    "    #tokens: Tensor of tokens indices (used to idx an embedding) => because a batch contains multiple\n",
    "    #sentences with varying # of tokens, all tokens tensors are padded with zeros \n",
    "    #shape: (batch_size (#sentences), max_seq_len)\n",
    "    #print(batch['tokens'])#dict with only 'tokens'\n",
    "    print(f\"Tokens shape {batch['tokens']['tokens'].shape}\")\n",
    "    print(f\"Tokenized sentence {batch['tokens']['tokens']}\")\n",
    "    #Defines the segments_ids (0 for first segment and 1 for second), can be used for NSP\n",
    "    #shape: (batch_size,max_seq_len)\n",
    "    print(f\"Segment ids shape: {batch['segment_ids'].shape}\")\n",
    "\n",
    "    #Dict with only wordnet\n",
    "    #Candidates: stores for multiple knowledge base, the entities detected using this knowledge base\n",
    "    for kb_key in batch['candidates'].keys():\n",
    "        kb_input = batch['candidates'][kb_key]\n",
    "    \n",
    "        #Stores for each detected entities, a list of candidate KB entities that correspond to it\n",
    "        #Priors: correctness probabilities estimated by the entity linker (sum to 1 (or 0 if padding) on axis 2)\n",
    "        #Adds 0 padding to axis 1 when there is less detected entities in the sentence than in the max sentence\n",
    "        #Adds 0 padding to axis 2 when there is less detected KB entities for an entity in the sentence than in the max candidate KB entities entity\n",
    "        #shape:(batch_size, max # detected entities, max # KB candidate entities)\n",
    "        print(f\"Candidate entity_priors shape: {kb_input['candidate_entity_priors'].shape}\")\n",
    "        #Ids of the KB candidate entities + 0 padding on axis 1 or 2 if necessary\n",
    "        #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "        print(f\"Candidate entities ids shape: {kb_input['candidate_entities']['ids'].shape}\")\n",
    "        #Spans of which sequence of tokens correspond to an entity in the sentence, eg: [1,2] for Michael Jackson (both bounds are included)\n",
    "        #Padding with [-1,-1] when no more detected entities\n",
    "        #shape: (batch_size, max # detected entities, 2)\n",
    "        print(f\"Candidate span shape: {kb_input['candidate_spans'].shape}\")\n",
    "\n",
    "        #For each sentence entity, indicate to which segment ids it corresponds to\n",
    "        #shape: (batch_size, max # detected entities)\n",
    "        print(f\"Candidate segments_ids shape: {kb_input['candidate_segment_ids'].shape}\")\n",
    "\n",
    "    model_output = wiki_model(**batch)\n",
    "    \n",
    "    print(f\"\\nOutput\\n\")\n",
    "    for kb_key in model_output.keys():\n",
    "        if(kb_key in ['wiki','wordnet']):\n",
    "            kb_output = model_output[kb_key]\n",
    "            #Span attention layers scores for wordnet KB\n",
    "            #shape: (batch_size,?,max_seq_len,max # detected entities)\n",
    "            print(f\"Output wordnet entity_attention_probs shape: {kb_output['entity_attention_probs'].shape}\")\n",
    "            #Entity linker score for each text entity and possible KB entity, -1.0000e+04 padding in case of no score\n",
    "            #shape: (batch_size, max # detected entities, max # KB candidate entities)\n",
    "            print(f\"Output wordnet linking_scores shape: {kb_output['linking_scores'].shape}\")\n",
    "        \n",
    "    #Scalar indicating loss over this batch (0 if not training?)\n",
    "    print(f\"Output loss: {model_output['loss']}\")\n",
    "\n",
    "    #Final CLS embedding for each sentence of batch\n",
    "    # shape: (batch_size, hidden_size) \n",
    "    print(f\"Pooled output shape: {model_output['pooled_output'].shape}\")\n",
    "    #For each tokens, its final embeddings\n",
    "    #Important!!!, still predicts something for 0 padded tokens => ignore (or 0 padding <=> MASK???)\n",
    "    print(f\"Contextual embeddings: {model_output['contextual_embeddings'].shape}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dd83c0049ca90416432c7bf31543fa4a43c6d1cbcb766d79372bfcb46406b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
