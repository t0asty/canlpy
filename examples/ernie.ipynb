{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding\n",
    "import tagme\n",
    "from canlpy.helpers.ernie_helpers import load_name_to_QID,load_QID_to_eid,process_sentences\n",
    "\n",
    "from canlpy.core.components.tokenization import BertTokenizer\n",
    "from canlpy.core.models.ernie.model import ErnieForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "KNOWLEDGE_DIR = '../canlpy/knowledge/ernie/'\n",
    "PRE_TRAINED_DIR = '../canlpy/pretrained_models/ernie/ernie_base/'\n",
    "\n",
    "NAME_TO_QID_FILE = KNOWLEDGE_DIR+ 'entity_map.txt'\n",
    "QID_TO_EID_FILE = KNOWLEDGE_DIR+ 'entity2id.txt'\n",
    "EID_TO_VEC_FILE = PRE_TRAINED_DIR + 'entity2vec.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model, _ = ErnieForMaskedLM.from_pretrained(PRE_TRAINED_DIR)\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2040, 2001, 3958, 27227, 1029, 102, 3958, 103, 2001, 1037, 103, 103, 1012, 102]\n",
      "predicted_token for index 8 is henson\n",
      "predicted_token for index 11 is popular\n",
      "predicted_token for index 12 is actor\n"
     ]
    }
   ],
   "source": [
    "#Suppose to predict hensen for idx 8\n",
    "def eval_sentence(text_a,text_b,model,tokenizer,masked_indices):\n",
    "\n",
    "    tokens_tensor,ents_tensor,ent_mask,segments_tensors = process_sentences(text_a,text_b,masked_indices,name_to_QID,QID_to_eid,eid_to_embeddings,tokenizer,device=device)\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, ents_tensor, ent_mask, segments_tensors)\n",
    "\n",
    "        # confirm we were able to predict 'henson'\n",
    "        for masked_index in masked_indices:\n",
    "            predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "            print(f\"predicted_token for index {masked_index} is {predicted_token}\")\n",
    "\n",
    "#Load pre-trained model tokenizer (vocabulary)\n",
    "#Special tokenizer for text and entities\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_DIR)\n",
    "\n",
    "#Eg: 'Northern Ireland': 'Q26'\n",
    "name_to_QID = load_name_to_QID(NAME_TO_QID_FILE)\n",
    "#Eg: {'Q11456633': 4525438, 'Q8863973': 1628631}\n",
    "QID_to_eid = load_QID_to_eid(QID_TO_EID_FILE)\n",
    "\n",
    "eid_to_embeddings = torch.load(EID_TO_VEC_FILE)\n",
    "#Creats a dictionnary of entity index->embeddings\n",
    "eid_to_embeddings = Embedding.from_pretrained(eid_to_embeddings)\n",
    "\n",
    "text_a = \"Who was Jim Henson ? \"\n",
    "text_b = \"Jim Henson was a puppeteer .\"\n",
    "\n",
    "#tokens_tensor,ents_tensor,ent_mask,segments_tensors = process_sentences(text_a,text_b,masked_indices,name_to_QID,QID_to_eid,tokenizer)\n",
    "#tokens: ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '.', '[SEP]']\n",
    "masked_indices = [8,11,12]#henson, puppet, ##eer\n",
    "eval_sentence(text_a,text_b,model,tokenizer,masked_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.word_embeddings.weight:torch.Size([30522, 768])\n",
      "model.embeddings.position_embeddings.weight:torch.Size([512, 768])\n",
      "model.embeddings.token_type_embeddings.weight:torch.Size([2, 768])\n",
      "model.embeddings.LayerNorm.weight:torch.Size([768])\n",
      "model.embeddings.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.0.attention.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.0.attention.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.0.attention.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.0.attention.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.0.attention.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.0.attention.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.0.attention.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.0.dense_intermediate.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.0.dense_intermediate.bias:torch.Size([3072])\n",
      "model.encoder.layer.0.skip_layer_out.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.0.skip_layer_out.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.0.skip_layer_out.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.0.skip_layer_out.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.1.attention.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.1.attention.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.1.attention.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.1.attention.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.1.attention.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.1.attention.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.1.attention.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.1.dense_intermediate.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.1.dense_intermediate.bias:torch.Size([3072])\n",
      "model.encoder.layer.1.skip_layer_out.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.1.skip_layer_out.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.1.skip_layer_out.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.1.skip_layer_out.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.2.attention.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.2.attention.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.2.attention.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.2.attention.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.2.attention.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.2.attention.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.2.attention.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.2.dense_intermediate.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.2.dense_intermediate.bias:torch.Size([3072])\n",
      "model.encoder.layer.2.skip_layer_out.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.2.skip_layer_out.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.2.skip_layer_out.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.2.skip_layer_out.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.3.attention.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.3.attention.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.3.attention.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.3.attention.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.3.attention.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.3.attention.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.3.attention.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.3.dense_intermediate.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.3.dense_intermediate.bias:torch.Size([3072])\n",
      "model.encoder.layer.3.skip_layer_out.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.3.skip_layer_out.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.3.skip_layer_out.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.3.skip_layer_out.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.4.attention.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.4.attention.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.4.attention.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.4.attention.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.4.attention.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.4.attention.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.4.attention.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.4.dense_intermediate.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.4.dense_intermediate.bias:torch.Size([3072])\n",
      "model.encoder.layer.4.skip_layer_out.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.4.skip_layer_out.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.4.skip_layer_out.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.4.skip_layer_out.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.5.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.5.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.5.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.5.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.5.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.5.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.5.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.5.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.5.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.5.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.5.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.5.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.5.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.5.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.5.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.5.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.5.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.5.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.5.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.6.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.6.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.6.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.6.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.6.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.6.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.6.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.6.attention_ent.multi_head_attention.query.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.6.attention_ent.multi_head_attention.query.bias:torch.Size([100])\n",
      "model.encoder.layer.6.attention_ent.multi_head_attention.key.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.6.attention_ent.multi_head_attention.key.bias:torch.Size([100])\n",
      "model.encoder.layer.6.attention_ent.multi_head_attention.value.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.6.attention_ent.multi_head_attention.value.bias:torch.Size([100])\n",
      "model.encoder.layer.6.attention_ent.skip_layer.dense.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.6.attention_ent.skip_layer.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.6.attention_ent.skip_layer.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.6.attention_ent.skip_layer.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.6.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.6.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.6.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.6.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.6.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.6.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.6.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.6.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.6.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.6.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.6.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.6.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.7.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.7.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.7.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.7.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.7.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.7.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.7.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.7.attention_ent.multi_head_attention.query.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.7.attention_ent.multi_head_attention.query.bias:torch.Size([100])\n",
      "model.encoder.layer.7.attention_ent.multi_head_attention.key.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.7.attention_ent.multi_head_attention.key.bias:torch.Size([100])\n",
      "model.encoder.layer.7.attention_ent.multi_head_attention.value.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.7.attention_ent.multi_head_attention.value.bias:torch.Size([100])\n",
      "model.encoder.layer.7.attention_ent.skip_layer.dense.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.7.attention_ent.skip_layer.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.7.attention_ent.skip_layer.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.7.attention_ent.skip_layer.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.7.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.7.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.7.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.7.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.7.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.7.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.7.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.7.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.7.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.7.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.7.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.7.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.8.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.8.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.8.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.8.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.8.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.8.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.8.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.8.attention_ent.multi_head_attention.query.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.8.attention_ent.multi_head_attention.query.bias:torch.Size([100])\n",
      "model.encoder.layer.8.attention_ent.multi_head_attention.key.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.8.attention_ent.multi_head_attention.key.bias:torch.Size([100])\n",
      "model.encoder.layer.8.attention_ent.multi_head_attention.value.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.8.attention_ent.multi_head_attention.value.bias:torch.Size([100])\n",
      "model.encoder.layer.8.attention_ent.skip_layer.dense.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.8.attention_ent.skip_layer.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.8.attention_ent.skip_layer.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.8.attention_ent.skip_layer.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.8.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.8.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.8.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.8.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.8.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.8.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.8.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.8.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.8.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.8.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.8.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.8.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.9.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.9.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.9.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.9.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.9.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.9.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.9.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.9.attention_ent.multi_head_attention.query.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.9.attention_ent.multi_head_attention.query.bias:torch.Size([100])\n",
      "model.encoder.layer.9.attention_ent.multi_head_attention.key.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.9.attention_ent.multi_head_attention.key.bias:torch.Size([100])\n",
      "model.encoder.layer.9.attention_ent.multi_head_attention.value.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.9.attention_ent.multi_head_attention.value.bias:torch.Size([100])\n",
      "model.encoder.layer.9.attention_ent.skip_layer.dense.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.9.attention_ent.skip_layer.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.9.attention_ent.skip_layer.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.9.attention_ent.skip_layer.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.9.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.9.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.9.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.9.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.9.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.9.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.9.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.9.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.9.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.9.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.9.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.9.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.10.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.10.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.10.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.10.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.10.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.10.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.10.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.10.attention_ent.multi_head_attention.query.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.10.attention_ent.multi_head_attention.query.bias:torch.Size([100])\n",
      "model.encoder.layer.10.attention_ent.multi_head_attention.key.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.10.attention_ent.multi_head_attention.key.bias:torch.Size([100])\n",
      "model.encoder.layer.10.attention_ent.multi_head_attention.value.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.10.attention_ent.multi_head_attention.value.bias:torch.Size([100])\n",
      "model.encoder.layer.10.attention_ent.skip_layer.dense.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.10.attention_ent.skip_layer.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.10.attention_ent.skip_layer.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.10.attention_ent.skip_layer.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.10.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.10.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.10.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.10.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.10.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.10.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.10.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.10.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.10.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.10.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.10.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.10.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.11.attention_tokens.multi_head_attention.query.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention_tokens.multi_head_attention.query.bias:torch.Size([768])\n",
      "model.encoder.layer.11.attention_tokens.multi_head_attention.key.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention_tokens.multi_head_attention.key.bias:torch.Size([768])\n",
      "model.encoder.layer.11.attention_tokens.multi_head_attention.value.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention_tokens.multi_head_attention.value.bias:torch.Size([768])\n",
      "model.encoder.layer.11.attention_tokens.skip_layer.dense.weight:torch.Size([768, 768])\n",
      "model.encoder.layer.11.attention_tokens.skip_layer.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.11.attention_tokens.skip_layer.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.11.attention_tokens.skip_layer.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.11.attention_ent.multi_head_attention.query.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.11.attention_ent.multi_head_attention.query.bias:torch.Size([100])\n",
      "model.encoder.layer.11.attention_ent.multi_head_attention.key.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.11.attention_ent.multi_head_attention.key.bias:torch.Size([100])\n",
      "model.encoder.layer.11.attention_ent.multi_head_attention.value.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.11.attention_ent.multi_head_attention.value.bias:torch.Size([100])\n",
      "model.encoder.layer.11.attention_ent.skip_layer.dense.weight:torch.Size([100, 100])\n",
      "model.encoder.layer.11.attention_ent.skip_layer.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.11.attention_ent.skip_layer.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.11.attention_ent.skip_layer.LayerNorm.bias:torch.Size([100])\n",
      "model.encoder.layer.11.fusion.dense_intermediate_tokens.weight:torch.Size([3072, 768])\n",
      "model.encoder.layer.11.fusion.dense_intermediate_tokens.bias:torch.Size([3072])\n",
      "model.encoder.layer.11.fusion.dense_intermediate_ent.weight:torch.Size([3072, 100])\n",
      "model.encoder.layer.11.fusion.dense_intermediate_ent.bias:torch.Size([3072])\n",
      "model.encoder.layer.11.fusion.skip_layer_tokens.dense.weight:torch.Size([768, 3072])\n",
      "model.encoder.layer.11.fusion.skip_layer_tokens.dense.bias:torch.Size([768])\n",
      "model.encoder.layer.11.fusion.skip_layer_tokens.LayerNorm.weight:torch.Size([768])\n",
      "model.encoder.layer.11.fusion.skip_layer_tokens.LayerNorm.bias:torch.Size([768])\n",
      "model.encoder.layer.11.fusion.skip_layer_ent.dense.weight:torch.Size([100, 3072])\n",
      "model.encoder.layer.11.fusion.skip_layer_ent.dense.bias:torch.Size([100])\n",
      "model.encoder.layer.11.fusion.skip_layer_ent.LayerNorm.weight:torch.Size([100])\n",
      "model.encoder.layer.11.fusion.skip_layer_ent.LayerNorm.bias:torch.Size([100])\n",
      "model.pooler.dense.weight:torch.Size([768, 768])\n",
      "model.pooler.dense.bias:torch.Size([768])\n",
      "cls.predictions.bias:torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight:torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias:torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight:torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias:torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}:{param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dd83c0049ca90416432c7bf31543fa4a43c6d1cbcb766d79372bfcb46406b7f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
